{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "# Force matplotlib to not use any Xwindows backend.\n",
    "# matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Sequential, model_from_json, load_model\n",
    "from keras.layers.core import Dense, Dropout, Flatten, Activation, SpatialDropout2D, Reshape, Lambda\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import ELU, PReLU, LeakyReLU\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from scipy.io import wavfile\n",
    "import pdb\n",
    "import scipy.io\n",
    "import librosa\n",
    "import os\n",
    "from os.path import join as ojoin\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "from keras.layers import CuDNNGRU, CuDNNLSTM\n",
    "import time  \n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import argparse\n",
    "import random\n",
    "import theano\n",
    "# os.environ['KERAS_BACKEND'] = 'theano'\n",
    "# import theano.tensor as T\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import TensorBoard\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "# from keras.utils import to_categorical,plot_model\n",
    "from ipykernel import kernelapp as app\n",
    "from tensorflow.keras import regularizers\n",
    "tf.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "else:\n",
    "    print(\"No GPU found\")\n",
    "config = tf.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.Session(config=config)\n",
    "KTF.set_session(session)\n",
    "random.seed(999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filepaths(directory):\n",
    "    \"\"\"\n",
    "    This function will generate the file names in a directory \n",
    "    tree by walking the tree either top-down or bottom-up. For each \n",
    "    directory in the tree rooted at directory top (including top itself), \n",
    "    it yields a 3-tuple (dirpath, dirnames, filenames).\n",
    "    \"\"\"\n",
    "    file_paths = []  # List which will store all of the full filepaths.\n",
    "\n",
    "    # Walk the tree.\n",
    "\n",
    "    for root, directories, files in os.walk(directory):\n",
    "\n",
    "        for filename in files:\n",
    "            if filename.endswith('.wav'):\n",
    "            # Join the two strings in order to form the full filepath.\n",
    "                filepath = os.path.join(root, filename)\n",
    "                file_paths.append(filepath)  # Add it to the list.\n",
    "                # pdb.set_trace()\n",
    "\n",
    "    return file_paths  # Self-explanatory.\n",
    "mixed_file=get_filepaths('mixed_all_snr')\n",
    "cleaned_file=get_filepaths('clean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#整理檔案路徑排序\n",
    "#確認乾淨的答案有對應到混音完的檔案\n",
    "clean_files=[]\n",
    "for i in mixed_file:\n",
    "    clean_file='_'.join(i.split('\\\\')[1].split('_')[:3])+'.wav'\n",
    "    for j in cleaned_file:\n",
    "#         print(j)\n",
    "        if clean_file == j.split('\\\\')[-1]:\n",
    "            clean_files.append(j)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##切分train-test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(mixed_file,clean_files,test_size=0.33, random_state=42)    \n",
    "Train_Noisy_lists=X_train\n",
    "Train_Clean_paths= y_train\n",
    "\n",
    "Test_Noisy_lists  = X_test\n",
    "Test_Clean_paths = y_test\n",
    "          \n",
    "Num_testdata=len(Test_Noisy_lists)   \n",
    "Num_traindata=len(Train_Noisy_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_generator(noisy_list, clean_path):\n",
    "    index=0\n",
    "    while True:\n",
    "        noisy, rate  = librosa.load(noisy_list[index],sr=16000) \n",
    "        # print(noisy_list[index],index)\n",
    "        # pdb.set_trace()\n",
    "\n",
    "        D=librosa.stft(noisy,n_fft=512,hop_length=256,win_length=512,center=False)\n",
    "        noisy = np.abs(D)\n",
    "        #轉向因為高固定 但長度不同\n",
    "        noisy = noisy.T\n",
    "        noisy=np.reshape(noisy,(1,np.shape(noisy)[0],np.shape(noisy)[1]))\n",
    "        noisy_shi=np.angle(D)\n",
    "\n",
    "\n",
    "        clean, rate  =librosa.load(clean_path[index],sr=16000) \n",
    "        D=librosa.stft(clean,n_fft=512,hop_length=256,win_length=512,center=False)\n",
    "        clean = np.abs(D)\n",
    "        #轉向因為高固定 但長度不同\n",
    "        clean = clean.T\n",
    "        clean=np.reshape(clean,(1,np.shape(clean)[0],np.shape(clean)[1]))\n",
    "        clean_shi=np.angle(D)\n",
    "\n",
    "        index += 1\n",
    "        if index == len(noisy_list):\n",
    "            index = 0\n",
    "\n",
    "        yield noisy, clean\n",
    "\n",
    "def val_data_generator(noisy_list, clean_path):\n",
    "    index=0\n",
    "    while True:\n",
    "        noisy, rate  = librosa.load(noisy_list[index],sr=16000)       \n",
    "        D=librosa.stft(noisy,n_fft=512,hop_length=256,win_length=512,center=False)\n",
    "        noisy = np.abs(D)\n",
    "        #轉向因為高固定 但長度不同\n",
    "        noisy = noisy.T\n",
    "        noisy=np.reshape(noisy,(1,np.shape(noisy)[0],np.shape(noisy)[1]))\n",
    "        noisy_shi=np.angle(D)\n",
    "          \n",
    "        clean, rate  =librosa.load(clean_path[index],sr=16000) \n",
    "        D=librosa.stft(clean,n_fft=512,hop_length=256,win_length=512,center=False)\n",
    "        clean = np.abs(D)\n",
    "        #轉向因為高固定 但長度不同\n",
    "        clean = clean.T\n",
    "        clean=np.reshape(clean,(1,np.shape(clean)[0],np.shape(clean)[1]))\n",
    "        clean_shi=np.angle(D)\n",
    "\n",
    "\n",
    "        index += 1\n",
    "        if index == len(noisy_list):\n",
    "            index = 0\n",
    "          \n",
    "        yield noisy, clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print('model building...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(CuDNNLSTM(80,  return_sequences=True, input_shape=(None,257)))\n",
    "model.add(CuDNNLSTM(80,  return_sequences=True))\n",
    "model.add(Dense(257,activation='tanh'))\n",
    "\n",
    "model.add(LeakyReLU())\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=5\n",
    "batch_size=1\n",
    "# Adam=tf.keras.optimizers.Adam(lr=0.01)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "    \n",
    "with open('{}.json'.format('firsttry'),'w') as f:    # save the model\n",
    "    f.write(model.to_json()) \n",
    "checkpointer = ModelCheckpoint(filepath='{}.hdf5'.format('firsttry'), verbose=1, save_best_only=True, mode='min')  \n",
    "\n",
    "print ('training...')\n",
    "\n",
    "g1 = train_data_generator(Train_Noisy_lists, Train_Clean_paths)\n",
    "g2 = val_data_generator(Test_Noisy_lists, Test_Clean_paths)\n",
    "\n",
    "tbCallBack = TensorBoard(log_dir='./logs',  # log 目录\n",
    "                 histogram_freq=0,  # 按照何等频率（epoch）来计算直方图，0为不计算\n",
    "#                  batch_size=32,     # 用多大量的数据计算直方图\n",
    "                 write_graph=True,  # 是否存储网络结构图\n",
    "                 write_grads=True, # 是否可视化梯度直方图\n",
    "                 write_images=True,# 是否可视化参数\n",
    "                 embeddings_freq=0, \n",
    "                 embeddings_layer_names=None, \n",
    "                 embeddings_metadata=None)\n",
    "hist=model.fit_generator(g1,\n",
    "                         samples_per_epoch=Num_traindata,\n",
    "                        # samples_per_epoch=50, \n",
    "                         nb_epoch=epoch, \n",
    "                         verbose=1,\n",
    "                         validation_data=g2,\n",
    "                         nb_val_samples=Num_testdata,\n",
    "                         max_q_size=1, \n",
    "                         nb_worker=1,\n",
    "                         pickle_safe=False,\n",
    "                         )                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sav=model.save('mixed_pos_snr_raw_LSTMx2_5epoch.h5')\n",
    "sav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## 畫Loss圖\n",
    "\n",
    "# # plotting the learning curve\n",
    "TrainERR=hist.history['loss']\n",
    "ValidERR=hist.history['val_loss']\n",
    "print ('@%f, Minimun error:%f, at iteration: %i' % (hist.history['val_loss'][epoch-1], np.min(np.asarray(ValidERR)),np.argmin(np.asarray(ValidERR))+1))\n",
    "# print 'drawing the training process...'\n",
    "plt.clf()\n",
    "plt.figure(4)\n",
    "plt.plot(range(1,epoch+1),TrainERR,'b',label='TrainERR')\n",
    "plt.plot(range(1,epoch+1),ValidERR,'r',label='ValidERR')\n",
    "plt.xlim([1,epoch])\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('error')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.savefig('Learning_curve_{}.png', dpi=150)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print ('The code for this file ran for %.2fm' % ((end_time - start_time) / 60.))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Test_Noisy_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maxv = np.iinfo(np.int16).max \n",
    "for path in Test_Noisy_lists[:10]: # Ex: /mnt/hd-02/avse/testing/noisy/engine/1dB/1.wav\n",
    "    S=path.split('\\\\') \n",
    "    # noise=S[-3]\n",
    "    # dB=S[-2]\n",
    "    wave_name=S[1]\n",
    "    noisy, rate  = librosa.load(path,sr=16000) \n",
    "    noisy=noisy.astype('float32')\n",
    "    # print(noisy_list[index],index)\n",
    "    # pdb.set_trace()\n",
    "    D=librosa.stft(noisy,n_fft=512,hop_length=256,win_length=512,center=False)\n",
    "    noisy = np.abs(D)\n",
    "    #轉向因為高固定 但長度不同\n",
    "    noisy = noisy.T\n",
    "    print(noisy.shape)\n",
    "    noisy_shi=np.angle(D)\n",
    "\n",
    "    noisy=np.reshape(noisy,(1,np.shape(noisy)[0],np.shape(noisy)[1]))\n",
    "    print(noisy.shape)\n",
    "#     print('reshape====>',noisy.shape)\n",
    "    enhanced=np.squeeze( model.predict(noisy, batch_size=1)).T\n",
    "#     print(enhanced.shape)\n",
    "    Rec = np.multiply(enhanced, np.exp(1j * noisy_shi))\n",
    "    enhanced=librosa.istft(Rec,hop_length=256,win_length=512,center=False)\n",
    "#     enhanced=enhanced/np.max(abs(enhanced))\n",
    "    enhanced=enhanced* maxv\n",
    "    enhanced_2=enhanced.astype('int16')\n",
    "#     enhanced=enhanced/2**15\n",
    "    wavfile.write(os.path.join(\"c:\\\\test\", wave_name),16000,(enhanced).astype(np.int16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 播放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = tf.contrib.keras.models.load_model('D:/result/mixed_pos_snr_STFT_GRUx3_tanh_5epoch_ok/mixed_pos_snr_STFT_GRUx3_tanh_5epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "path='C:/Users/hsiaoen/Desktop/jin_T03-8_20200620.wav'\n",
    "S=path.split('/') \n",
    "wave_name=S[1]\n",
    "noisy, sr = librosa.load(path,sr=16000)\n",
    "noisy=noisy.astype('float32')\n",
    "D=librosa.stft(noisy,n_fft=512,hop_length=256,win_length=512,center=False)\n",
    "noisy = np.abs(D)\n",
    "noisy = noisy.T\n",
    "print(noisy.shape)\n",
    "noisy_shi=np.angle(D)\n",
    "noisy=np.reshape(noisy,(1,np.shape(noisy)[0],np.shape(noisy)[1]))\n",
    "enhanced=np.squeeze( model.predict(noisy, batch_size=1)).T\n",
    "Rec = np.multiply(enhanced, np.exp(1j * noisy_shi))\n",
    "Rec[100:-100]*=100\n",
    "Rec=(Rec-np.mean(Rec))//np.std(Rec)\n",
    "enhanced=librosa.istft(Rec,hop_length=256,win_length=512,center=False)\n",
    "enhanced_2=enhanced.astype('int16')\n",
    "plt.clf()\n",
    "plt.plot(enhanced_2)\n",
    "ipd.Audio(enhanced_2,rate=16000) #播放\n",
    "# wavfile.write(os.path.join(\"c:/test\",wave_name),16000,(enhanced).astype(np.int16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyhht.visualization import plot_imfs\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "\n",
    "path='C:/Users/hsiaoen/Desktop/jin_T03-8_20200620_fireengine_starting_snr4.wav'\n",
    "data , sr = librosa.load(path,sr=16000) \n",
    "# data = data/abs(data).max()\n",
    "data[100:-100]*=100\n",
    "plt.plot(data)\n",
    "ipd.Audio(data,rate=16000) #播放\n",
    "librosa.output.write_wav('jin_T03-8_20200620_fireengine_starting_snr4.wav', data, sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhht.visualization import plot_imfs\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "\n",
    "path='C:/Users/hsiaoen/Desktop/denoise_jin_T03-8_20200620_fireengine_starting_snr4 (1).wav'\n",
    "data , sr = librosa.load(path,sr=16000) \n",
    "# data = data/abs(data).max()\n",
    "data[100:-100]*=100\n",
    "plt.plot(data)\n",
    "ipd.Audio(data,rate=16000) #播放"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
